log_file: train_log

random_seed: 2022

use_gpu: True
gpu_ids: 1
# gpu_ids: 0,1 # 自动加载多卡

dataset:
  train: # 若多卡，只有训练阶段进行并行
    data_path: "data/train.txt"
    type: json # json/jsonl/text  json 是整个文件为一个json or jsonl是每行为一个json or text
    reader:
      type: XXX_reader
      tokenizer: XXX_tokenizer
      vocab_path: "pretrain_model/vocab.txt"
      padding: max_length
      max_seq_len: 512
    batch_size: 16 # 若多卡，实际batch_size = n_gpus * batch_size
  dev:
    data_path: "data/dev.txt"
    batch_size: 16 # 若不设置，则为train的batch size
  test:
    data_path: "data/test.txt"
    batch_size: 16 # 若不设置，则为train的batch size

model:
  # 模型参数可随意添加
  type: XXX_model
  encoder: # encoder主要针对加载预训练模型和进行分层学习率训练
    type: bert
    pretrained_model_dir: "pretrain_model"
  weight_path: "pretrain_model/model.pth" # 若存在则加载权重

optimizer:
  type: adamw
  lr: 1e-4 # 任务层学习率
  encoder_lr: 1e-5 # 嵌入层学习率
  encoder_name: encoder # 嵌入层名称，不设置默认为encoder
  weight_decay: 0.01
  lr_scheduler: linear # linear/cosine/cosine_with_restarts/polynomial/constant/constant_with_warmup
  warmup: 0.2

trainer:
  epoch: 10
  eval_epoch: 1
  test_epoch: 1
  save_epoch: 1 # 若没有验证集下，多少轮保存一次模型
  model_sort_key: F1
  early_stopping: False # 是否早停
  max_model_num: 5 # 最优模型保存数量
  only_save_model_weight: False # 是否只保存模型的权重
  use_fp16: False # 是否开启混合精度训练
  use_clip_norm: False # 是否使用梯度裁剪 若为数值则默认启动梯度裁剪并设置对应的clip norm
  use_fgm: False # 是否使用对抗训练FGM 若为小数则默认启动FGM并设置对应的epsilon，需要在dltk/modules/fgm中修改对应的权重名称
  use_ema: False # 是否使用EMA 若为小数则默认启动EMA并设置对应的decay
  output_path: "training_output"
  save_evaluations: True # 是否保存验证集指标
  save_checkpoints: True # 是否保存模型
  save_predictions: True # 是否保存预测结果
